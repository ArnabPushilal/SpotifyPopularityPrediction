# -*- coding: utf-8 -*-
"""Spot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pXks9P9tyVPxYEF30Pl6BriFKiyBuS8Q
"""

pip install spotipy

from spotipy.oauth2 import SpotifyOAuth
from google.colab import files
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.utils import resample
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import log_loss
from matplotlib import pyplot
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
import spotipy
import math
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression
import pandas as pd
from sklearn.metrics import accuracy_score
from scipy import stats   
from scipy.stats import shapiro
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import timeit
import time
import seaborn as sns
import matplotlib.pyplot as plt
from numpy import set_printoptions
from sklearn.feature_selection import SelectFromModel
import ast 
import sys
import json
from spotipy.oauth2 import SpotifyClientCredentials
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

client_id = '238e859d309249ccbe58da9c6c0e6b54'
client_secret = '2a0d395ac7c5418ab7de8ca760875e22'

client_credentials_manager = SpotifyClientCredentials(client_id, client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

# Defining a function to return audio features for a particular track id
def audio_feats(track_id):
    a=sp.audio_features(track_id)
    return a

# Returns audio_features to prepare it for a dataframe
def feat_value_extract(Data):
 acousticness=[]
 danceability=[]
 analysis_url=[]
 duration_ms=[]
 energy=[]
 track_id=[]
 instrumentalness=[]
 key=[]
 liveness=[]
 loudness=[]
 mode=[]
 speechiness=[]
 tempo=[]
 time_signature=[]
 track_href=[]
 uri=[]
 valence=[]

 for i in Data['track_id']:
  if i != [None]:
   acousticness.append(i[0]['acousticness'])
   danceability.append(i[0]['danceability'])
   analysis_url.append(i[0]['analysis_url'])
   duration_ms.append(i[0]['duration_ms'])
   energy.append(i[0]['energy'])
   track_id.append(i[0]['id'])
   instrumentalness.append(i[0]['instrumentalness'])
   key.append(i[0]['key'])
   liveness.append(i[0]['liveness'])
   loudness.append(i[0]['loudness'])
   mode.append(i[0]['mode'])
   speechiness.append(i[0]['speechiness'])
   tempo.append(i[0]['tempo'])
   time_signature.append(i[0]['time_signature'])
   track_href.append(i[0]['track_href'])
   uri.append(i[0]['uri'])
   valence.append(i[0]['valence'])
 return acousticness,danceability,analysis_url,duration_ms,energy,track_id,instrumentalness,key,liveness,loudness,mode,speechiness,tempo,time_signature,track_href,uri, valence



# defining a function to query tracks by year range
def query(sample_size,yearold,yearnew):
 artist_name = []
 track_name = []
 popularity = []
 track_id = []
 available_markets=[]
 year=[]
 for j in range(yearold,yearnew+1): 
   for i in range(0,sample_size,50):
      track_results = sp.search(q=f"year:{j}", type='track', limit=50,offset=i)
      for i, t in enumerate(track_results['tracks']['items']):
         artist_name.append(t['artists'][0]['name'])
         track_name.append(t['name'])
         track_id.append(t['id'])
         available_markets.append(len(t['available_markets']))
         popularity.append(t['popularity'])
         year.append(j)

 return year,artist_name,track_name,track_id,available_markets,popularity

def createdata():
 Data=pd.DataFrame({'Artist':artist_name,'Track':track_name,'track_id':track_id,'NoOfMarkets':available_markets,'PopularityScore':popularity,'Year':year})
 return Data

def createdata_feats():
 Data=pd.DataFrame({'acousticness':acousticness ,'danceability':danceability,'analysis_url':analysis_url,'duration_ms':duration_ms,'energy':energy,'track_id':track_id,'instrumentalness':instrumentalness,'key':key,'liveness':liveness,'loudness':loudness,'mode':mode,'speechiness':speechiness,'tempo':tempo,'time_signature':time_signature,'track_href':track_href,'uri':uri,'valence':valence})
 return Data

# Creating the entire DataFrame , takes quite a while to run
start = timeit.default_timer()
year,artist_name,track_name,track_id,available_markets,popularity=query(2000,1980,2020)
DataFrame=createdata()
Data=pd.DataFrame(DataFrame['track_id'].map(lambda x:audio_feats(x)))
acousticness,danceability,analysis_url,duration_ms,energy,track_id,instrumentalness,key,liveness,loudness,mode,speechiness,tempo,time_signature,track_href,uri, valence=feat_value_extract(Data)
DataTrack=createdata_feats()
stop = timeit.default_timer()
execution_time = stop - start
print("Program Executed in "+str(execution_time))

# Converting to csv to save
DataFrame.to_csv('/drive/My Drive/SpotifyData/ArtistData1.csv')
DataTrack.to_csv('/drive/My Drive/SpotifyData/TrackData.csv')

from google.colab import  drive
drive.mount('/drive' )

DataTrack=pd.read_csv('/drive/My Drive/SpotifyData/TrackData.csv')
DataFrame=pd.read_csv('/drive/My Drive/SpotifyData/ArtistData1.csv')

# Merged Frame including Popularity & Audio Features
DataFinal=pd.merge(DataTrack,DataFrame,on='track_id',how='inner')
DataFinal

DataFinal.drop(['Unnamed: 0_x','Unnamed: 0_y'],axis=1,inplace=True)

DataFinal.describe(include='all')

# Defining a function to get rid of outliers 
def outlier(Data):
 Q1 = Data.quantile(0.25)
 Q3 = Data.quantile(0.75)
 IQR = Q3 - Q1
 data_clean = Data[~((Data < (Q1 - 1.5 * IQR)) |(Data > (Q3 + 1.5 * IQR))).any(axis=1)]
 print(data_clean.shape)
 return data_clean

# Year vs Avg Popularity, increasing by Year
fig=plt.figure(figsize=(10,10))
ax=sns.lineplot(data=DataFinal.groupby('Year')['PopularityScore'].mean())
ax.set_title("Year vs Average Popularity")
ax.set_xlabel('Year')

ax.set_ylabel('Average PopularityScore')
plt.savefig("Year_vs_AvgPopScore.png")
files.download("Year_vs_AvgPopScore.png")

#Dividing Data into Decades
DataEight=DataFinal.loc[(DataFinal['Year'] <= 1989) ]
DataNine= DataFinal.loc[(DataFinal['Year'] <=1999) & (DataFinal['Year']>=1990) ]
DataZero= DataFinal.loc[(DataFinal['Year'] <=2009) & (DataFinal['Year']>=2000) ]
DataTen = DataFinal.loc[(DataFinal['Year'] <=2019) & (DataFinal['Year']>=2010) ]

# Function for UniVariatePlots
def univariateplot(Data):
 fig=plt.figure(figsize=(20,20))
 rows=7
 cols=7
 DataNumerical=Data.drop(['track_href','uri','Track','track_id','analysis_url','Artist','Year'],axis=1)
 for i, feature in enumerate(DataNumerical.columns):
    ax=fig.add_subplot(rows,cols,i+1)
    sns.distplot(DataNumerical[feature],kde=False)
    ax.set_title(feature+" Distribution",color='red')
 fig.tight_layout()     
 plt.show()

univariateplot(DataTen)

# a lot of 0s in the 2020 + low popularity score, so will not use data from the 2020s
DataFinal[DataFinal['PopularityScore']==0].groupby(['Year']).count()

# float64 precision should not be needed
DataEight.dtypes

def func(x):
  if x>8:
    x=1
  else:
    x=0
  return x

#Function to Quantize Popularity Score on Popular -1 & Not Popular - 0
def PopQuant(Data):
 DataNumerical=Data.drop(['track_href','uri','Track','track_id','analysis_url','Artist','Year'],axis=1)
 DataNumerical['PopQuant']=pd.qcut(Data['PopularityScore'],10,labels=[0,1,2,3,4,5,6,7,8,9])
 out,bin=pd.qcut(DataNumerical['PopularityScore'],10,retbins=True)
 print("Binned Popularity according to 10 quantiles",bin)
 DataNumerical['PopQuant']=DataNumerical['PopQuant'].map(lambda x: func(x))
 return DataNumerical

# Log transformation function / added arbriraty constants since data cannot be negative or 0
def transform(Data):
 Data['speechiness'], _ = stats.boxcox(Data['speechiness']+0.0000001)  
 Data['loudness'], _ = stats.boxcox(Data['loudness']-np.min(Data['loudness'])+0.0000001)  
 Data['liveness'] ,_ = stats.boxcox(Data['liveness']+0.0000001) 
 Data['instrumentalness'] ,_ = stats.boxcox(Data['instrumentalness']+0.0000001)
 Data['duration_ms'] ,_ = stats.boxcox(Data['duration_ms'])  
 Data['NoOfMarkets'] ,_ = stats.boxcox(Data['NoOfMarkets']+1)          
 xt_= (Data['speechiness'] - np.mean(Data['speechiness']) )/ np.std(Data['speechiness'])
 xt_1= (Data['loudness'] - np.mean(Data['loudness']) )/ np.std(Data['loudness'])
 xt_2= (Data['liveness'] - np.mean(Data['liveness']) )/ np.std(Data['liveness'])
 xt_3= (Data['instrumentalness'] - np.mean(Data['instrumentalness']) )/ np.std(Data['instrumentalness'])
 xt_4= (Data['duration_ms'] - np.mean(Data['duration_ms']) )/ np.std(Data['duration_ms'])
 xt_5= (Data['danceability'] - np.mean(Data['danceability']) )/ np.std(Data['danceability'])
 xt_6= (Data['energy'] - np.mean(Data['energy']) )/ np.std(Data['energy'])
 xt_7= (Data['valence'] - np.mean(Data['valence']) )/ np.std(Data['valence'])
 xt_8= (Data['NoOfMarkets'] - np.mean(Data['NoOfMarkets']) )/ np.std(Data['NoOfMarkets'])
 Data['speechiness']=xt_
 Data['loudness']=xt_1
 Data['liveness']=xt_2
 Data['instrumentalness']=xt_3
 Data['duration_ms'] = xt_4
 Data['danceability'] = xt_5
 Data['energy'] = xt_6
 Data['valence'] =xt_7
 Data['NoOfMarkets'] =xt_8
  

 return Data['speechiness'],Data['loudness'],Data['liveness'],Data['instrumentalness'],Data['duration_ms'],Data['danceability'],Data['energy'],Data['valence'],Data['NoOfMarkets']

# function for correlation heatmap
def heat_map(DataFinal,title):
 DataNumerical=PopQuant(DataFinal)
 print(title,":")

 f, ax = plt.subplots(figsize=(7, 7))
 corr_matrix=DataNumerical[['danceability','energy','loudness','valence','liveness','tempo','PopQuant','acousticness','duration_ms','instrumentalness']].corr()
 heatmap = sns.heatmap(corr_matrix,linewidths=.5,annot=True)
 plt.savefig(f'HeatMap_{title}.png' )
 files.download(f'HeatMap_{title}.png' )

 ax.set_yticklabels(corr_matrix.columns, rotation = 0)
 ax.set_xticklabels(corr_matrix.columns)
 ax.set_title(title)
 sns.set_style({'xtick.bottom': True}, {'ytick.left': True} )

def pairplot(Data,title):
 DataNumerical=PopQuant(Data)
 sns.pairplot(DataNumerical[['danceability','energy','loudness','valence','liveness','tempo','PopQuant','acousticness','instrumentalness']],hue='PopQuant',kind='reg')
 plt.savefig('PairPlot.png' )
 files.download('PairPlot.png' )

pairplot(DataTen,'10s')

heat_map(DataEight,'80s')
heat_map(DataNine,'90s')
heat_map(DataZero,'00s')
heat_map(DataTen,'10s')

#probability for categorical variables being popular or not
def pop_prob(Data):
 DataNumerical=PopQuant(Data)
 print(DataNumerical[['mode','PopQuant']].groupby('mode').mean().sort_values(by="PopQuant",ascending = False))
 print(DataNumerical[['time_signature','PopQuant']].groupby('time_signature').mean().sort_values(by="PopQuant",ascending = False))
 print(DataNumerical[['key','PopQuant']].groupby('key').mean().sort_values(by="PopQuant",ascending = False))

# 1/10 data is popular, so any more aggregrate than that could be an indication for popularity, Doesn't look like mode,time_signature or key is important
pop_prob(DataTen)

def DataNumerical(Data):
  DataNumerical=PopQuant(Data)
  y=DataNumerical['PopQuant']
  x=DataNumerical[['danceability','energy','loudness','valence','liveness','tempo','acousticness','duration_ms','instrumentalness']]
  return np.array(x),np.array(y)

def DataCategorical(data):
  DataCategorical=PopQuant(Data)
  y=DataCategorical['PopQuant']
  x=DataCategorical[['mode','time_signature','key']]
  return x,y

# Function to calculate one-way-anova
def calculateStat(Numerical , Categorical): 
    fValue={}
    pValue={}
    scaler = StandardScaler()
    Numerical = scaler.fit_transform(Numerical)
    column=['danceability','energy','loudness','valence','liveness','tempo','acousticness','duration_ms','instrumentalness']
    var_1 = Numerical[Categorical==1] 
    var_0 = Numerical[Categorical==0] 
    for i,col in enumerate(column):
     fValue[col], pValue[col] = stats.f_oneway(var_1[:,i], var_0[:,i])
    value=pd.DataFrame([fValue,pValue],index=['f','pValue'])
    return value

transform(DataTen)
x,y=DataNumerical(DataTen)
calculateStat(x,y)

def one_hot_encode(Data):
  Data = pd.get_dummies(Data, columns=['mode','time_signature','key'])
  return Data

def reduce_mem_usage(df):

    start_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            if str(col_type)[:3] == 'int':   
              df[col] = df[col].astype(np.int8)
            else:
              df[col] = df[col].astype(np.float16)
        else:
            df[col] = df[col].astype('category')

    end_mem = df.memory_usage().sum() / 1024**2
    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
    
    return df

# Prepare the training variables, for Randomforest it was not necessary to one hot encode ( or to scale, but I did that anyway)
def trainvar(Data,scaling,test_size,modeltype,forest):
 DataNi=PopQuant(Data)
 reduce_mem_usage(DataNi)
 DataY=DataNi['PopQuant']
 DataY_=DataNi['PopularityScore']
 if forest==True:
  DataX=DataNi
 else:
  DataX=one_hot_encode(DataNi)
 DataX=DataX.drop(['PopularityScore','PopQuant'],axis=1)
 if scaling == 'standard' :
  scaler_=StandardScaler()
  DataX_=scaler_.fit_transform(DataX)
 if scaling == 'minmax':
  scaler=MinMaxScaler(feature_range = (0,1))
  DataX_=scaler.fit_transform(DataX)
 else:
  DataX_=DataX
 if modeltype == 'regression':
  X_train, X_test, y_train, y_test = train_test_split(DataX_,DataY_ ,test_size=test_size)
 else:
  X_train, X_test, y_train, y_test = train_test_split(DataX_,DataY ,test_size=test_size)
 return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = trainvar(DataTen,'minmax',0.25,'regression',True)
y_train

# Function for confusion matrix
def conf(model,y_test,yHat):
 cm=confusion_matrix(y_test,yHat)
 print(classification_report(y_test,yHat))
 sns.heatmap(cm,linewidths=0.5,annot=True)

# To calculate feature importance
def feat_imp(model,Data,type):
 DataNi=PopQuant(Data)
 DataX=one_hot_encode(DataNi)
 DataX=DataX.drop(['PopularityScore','PopQuant'],axis=1)
 DataX_=PopQuant(Data)
 DataX_=DataX_.drop(['PopularityScore','PopQuant'],axis=1)
 if type == 'nonforest':
  importance = model.coef_[0]
 else:
  importance = model.feature_importances_
 for i,v in zip(DataX.columns,importance):
  print(i,' : %.5f'%(v))
 fig=plt.figure(figsize=(20,20))
 if type=='nonforest':
  plt.subplot(2,1,1)
  plt.bar(DataX.columns[:10],importance[:10])
  plt.savefig('feature_imp_1.png' )
  files.download('feature_imp_1.png' )
  plt.subplot(2,1,2)
  plt.bar(DataX.columns[10:30],importance[10:30])
  plt.tick_params(labelsize=5)
  plt.savefig('feature_imp_2.png' )
  files.download('feature_imp_2.png' )
 else:
  plt.bar(DataX_.columns,importance)
  plt.savefig('feature_imp.png' )
  files.download('feature_imp.png' )

# Function to calculate AUC of ROC
def ROC(X_test,X_train,y_train,y_test,clf,modelname):
 NS = [0 for _ in range(len(y_test))]
 modelprob = clf.predict_proba(X_test)
 modelprob=modelprob[:,1]
 modelprob_t = clf.predict_proba(X_train)
 modelprob_t=modelprob_t[:,1]
 ns_auc = roc_auc_score(y_test, NS )
 m_auc = roc_auc_score(y_test, modelprob)
 m_auc_t = roc_auc_score(y_train, modelprob_t)
 print('No Skill: ROC AUC=%.3f' % (ns_auc))
 print(modelname,': test ROC AUC=%.3f' % (m_auc))
 print(modelname,':train ROC AUC=%.3f' % (m_auc_t))
 m_fpr_,m_tpr_, _ = roc_curve(y_test, NS)
 m_fpr, m_tpr, _ = roc_curve(y_test, modelprob)
 m_fpr_t, m_tpr_t, _ = roc_curve(y_train, modelprob_t)
 fig=plt.figure(figsize=(10,10))
 plt.plot(m_fpr_, m_tpr_, linestyle='dashed', label='No Skill')
 plt.plot(m_fpr, m_tpr, marker='x', label='test')
 plt.plot(m_fpr_t, m_tpr_t, marker='.', label='train')
 plt.xlabel('False Positive Rate')
 plt.ylabel('True Positive Rate')
 plt.legend()
 plt.savefig(f'ROC_{modelname}.png' )
 files.download(f'ROC_{modelname}.png' )

 plt.show()

# Grid Search for best logistic Regression Model 
param= {
     'solver' : ['lbfgs', 'liblinear','sag','saga'],
     'C' : np.logspace(0.01, 3 , num=10)}

m_auc=0
best_model = LogisticRegression(class_weight='balanced',random_state=0)
accuracy=0
dict_param_={ }
for solver in param['solver']:
  for C in param['C']:
    model=LogisticRegression(solver=solver,C=C,random_state=0,max_iter=100,n_jobs=-1,class_weight='balanced')
    model.fit(X_train,y_train)
    y_pred=model.predict(X_test)
    curr_accuracy=accuracy_score(y_test,y_pred)
    modelprob=model.predict_proba(X_test)
    modelprob=modelprob[:,1]
    curr_m_auc = roc_auc_score(y_test, modelprob) 
    dict_param_[(solver,C)]=curr_m_auc
    print("for params :",solver," ",C," the ROC AUC is  : ",dict_param_[(solver,C)])
    if curr_m_auc > m_auc:
      m_auc=curr_m_auc
      best_model=model




best_model

y_hat=best_model.predict(X_test)
conf(best_model,y_test,y_hat)

ROC(X_test,X_train,y_train,y_test,best_model,'logistic')

feat_imp(best_model,DataTen,'nonforest')

# Random Forest, randomsearchcv
model = RandomForestClassifier(class_weight='balanced')
random_grid= {'bootstrap': [True, False],
 'max_depth': [ 60, 70, 80, 90, 100, None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 400, 600, 800, 1000],
 'criterion': ['gini','entropy']}

clf = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 5, verbose=2, random_state=42, n_jobs = -1) 
clf.fit(X_train,y_train)

yHat=clf.predict(X_test)
yhat=clf.predict_proba(X_test)

ROC(X_test,X_train,y_train,y_test,clf,'Random')

# Function to define regression accuracy metrics
def regacc(model,X,y):
 y_pred=model.predict(X)
 print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y, y_pred))
 print('Mean Squared Error (MSE):', metrics.mean_squared_error(y, y_pred))
 print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y,  y_pred)))
 mape = np.mean(np.abs((y - y_pred) / np.abs( y_pred)))
 print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2))
 print('Accuracy:', round(100*(1 - mape), 2))

# Function to plot distributions of popularity
def PopularityPlot(regr,X,y):
 yhat=regr.predict(X)
 plt.figure(figsize=(15, 15))
 ax1 = sns.distplot(y, hist=False, color="r", label="Actual Value")
 sns.distplot(yhat, hist=False, color="b", label="Fitted Values" , ax=ax1)
 plt.title('Actual vs Fitted Values for Popularity')
 plt.xlabel('Popularity')
 plt.savefig('Actual vs Fitted Values for Popularity.png')
 files.download('Actual vs Fitted Values for Popularity.png' )
 plt.show()
 plt.close()

# Random Search on Random forest 
random_grid={'bootstrap': [True, False],
 'max_depth': [2,5,10, 20, 30, 40, 50, 60, None],
 'max_features': ['auto', 'sqrt'],
 'n_estimators': [200, 400, 600, 800, 1000]}
regr = RandomForestRegressor()
regr = RandomizedSearchCV(estimator = regr, param_distributions = random_grid, cv = 3, verbose=2, random_state=42, n_jobs = -1) 
model=regr.fit(X_train,y_train)

model_.fit(X_train,y_train)
regacc(model_,X_test,y_test)
feat_imp(model_,DataTen,'forest')

PopularityPlot(model_,X_train,y_train)
PopularityPlot(model_,X_test,y_test)

## Grid search on ridge regression with polynominal features
X_train, X_test, y_train, y_test = trainvar(DataTen,'minmax',0.3,'regression')
degree_1 =[1,2,3]
alpha_1= [ 0.01,0.1,1,10,100,200,500,1000]
MSE=10000000
best_degree=-1
for degree in degree_1:
  pr=PolynomialFeatures(degree=degree)
  X_train_=pr.fit_transform(X_train)
  X_test_=pr.fit_transform(X_test)
  for alpha in alpha_1:  
    model=Ridge(alpha=alpha,fit_intercept=True)
    model.fit(X_train_,y_train)
    y_pred=model.predict(X_test_)
    y_pred_train=model.predict(X_train_)
    curr_MSE= metrics.mean_squared_error(y_test, y_pred)
    curr_MSE_train=metrics.mean_squared_error(y_train, y_pred_train)
    print("For degree:",degree,",alpha:",alpha,"the train MSE is",curr_MSE_train,"the test MSE is ",curr_MSE)
    if curr_MSE<MSE:
      MSE=curr_MSE
      best_model=model
      best_degree=degree

pr=PolynomialFeatures(degree=best_degree)
X_train_=pr.fit_transform(X_train)
X_test_=pr.fit_transform(X_test)

PopularityPlot(best_model,X_train_,y_train)
PopularityPlot(best_model,X_test,y_test)

# Getting the features for my own track
available_markets=0
track_results = sp.track('0y2Ga0Au2NZaRuIGEAiwTQ')
for i in track_results['album']['available_markets']:
  available_markets+=1

available_markets
DataFrame=pd.DataFrame(audio_feats('0y2Ga0Au2NZaRuIGEAiwTQ'))
DataFrame['NoOfMarkets']=available_markets